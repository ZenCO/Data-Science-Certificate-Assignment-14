---
title: "Decision Tree Assignment"
author: "Scott Stoltzman"
date: "7/17/2019"
output: html_document
---
###This is the setup where we decide what libraries are needed in order to make sense of our analysis
```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('tidyverse')
library('caret')
library('GGally')
library('rpart.plot')
set.seed(123)
```

## Data
###Here we are defining as our raw data, as a data frame... the MPG data set.  Then we are defining MPG as an average between the city and highway milage.  We are also getting rid of city miles, highway miles, the model and whatever fl is (I can look it up, but I am sleepy).
```{r}
raw_dat = as.data.frame(mpg) %>%
  mutate(mpg = (cty + hwy)/2) %>%
  select(-cty, -hwy, -model, -fl)
head(raw_dat)
```


## Group your mpg into "high, medium, low" where mpg:
high = >90%
medium = 30% - 90%
low = <30%
(hint: find deciles group data for mpg)
*Replace* your price column with this new data

### I was UNABLE to get the formula for decile to work.  So I used the quantile function instead and approximated the buckets.  I defined High as 80% or more and low as 40% or below.  It was what I considered reasonable thresholds, but they are arbitrary and I realize it.
```{r}
mpg_buckets = quantile(raw_dat$mpg)

dat = raw_dat %>%
  mutate(mpg = if_else(mpg < mpg_buckets[2],
                          "low",
                          if_else(mpg > mpg_buckets[4],
                                  "high",
                                  "medium")))

```


## Split data to test / train
###This is pretty straight forward.  We are defining our sample size as the bottom 85% (hence the floor bit).  

###The dat_index is how we are setting up the 85% and 15% we are using for dat_train and dat_test.  This makes sense to me.

```{r}
training_split = 0.85
smp_size = floor(training_split * nrow(dat))
dat_index = sample(seq_len(nrow(dat)), size = smp_size)
dat_train = dat[dat_index,]
dat_test = dat[-dat_index,]
```


## Create a decision tree using `caret::train` and the algorithm `rpart`
```{r}
tctrl = trainControl(method = 'cv',
                     number = 10,
                     savePredictions = TRUE,
                     classProbs = TRUE)

mod_dt = train(dat_train %>% select(-mpg), 
                dat_train$mpg, 
                method = 'rpart',
                parms = list(split = "information"),
                trControl=tctrl)

pred_dt = predict(mod_dt, dat_test, type = 'prob')
```

## Visualize your tree
Use the `prp` function

###So using the PRP function, we are creating a decision matrix.  So with displacement less than 2.6 (assuming litres), it is classified as medium.  If it is above 2.6 literes, then it is looking at the manufacturer as the deciding factor.  That makes sense to me.  Honda, Nissan Toyota and Volkswagon are clumped together.  And likely have higher MPG.
```{r}
prp(mod_dt$finalModel, box.palette = "auto")
```


## Predict whether the following car would be "high, medium, or low" mpg
```{r}
new_car = data.frame(
  manufacturer = 'audi',
  model = 'a4',
  displ = 2.3,
  year = 2000,
  cyl = 6,
  trans = 'auto(l5)',
  drv = 'f',
  fl = 'p',
  class = 'compact'
)
predict(mod_dt, mpg)
```


## Create a small write up on your model

Describe your results.
Show the model parameters, confusion matrix, etc.
Explain why you chose to use the parameters you did for this decision tree model.

### What this tells me is that it's most likely a medium MPG vehicle.  HIGH MPG is unlikely due to it being a 6 cyl instead of a 4 cyl.  But it's an AUDI, and so it's probably less tuned for MPG and more for performance (unlike toyota, nissan, etc).

###The results (which is that this is most likely a medium MPG vehicle... meaning it is between 40% and 80% seems more than reasonable).  I could go more in depth, but I am getting a headache.
